# MOSAIC-GPT Dense â€” Optimized for 2x RTX 3090 (24GB each)
#
# Architecture: MLA + SwiGLU (no MoE) + Decoupled RoPE + RMSNorm
# Same attention as mosaic_small but with a single dense SwiGLU FFN.
#
# Parameter count: ~122M (similar to GPT-2 124M, good baseline)
#
# VRAM estimate: ~1.2GB params+optimizer (FP16/FP32 mixed)
#   + ~4-6GB activations at batch_size=16, seq_len=1024
#   Very comfortable in 24GB per GPU.
#
# Effective batch size: 16 * 8 grad_accum * 2 GPUs = 256 tokens/step
# Launch with: --grad-accum 8

d_model: 768
n_layers: 12
vocab_size: 50257  # GPT-2 tokenizer

attention:
  type: mla
  n_heads: 12
  kv_compression_dim: 256
  q_compression_dim: 384
  rope_dim: 64
  dropout: 0.0

ffn:
  type: swiglu
  hidden_mult: 2.6667  # 8/3, gives hidden_dim=2048
  dropout: 0.0

position:
  type: decoupled_rope
  max_seq_len: 2048
  rope_base: 10000.0

norm:
  type: rmsnorm
  eps: 1.0e-6

output:
  type: tied

embed_dropout: 0.0
residual_dropout: 0.0

training:
  batch_size: 16         # Safe for 122M params on 24GB
  seq_len: 1024
  lr: 3.0e-4
  min_lr: 3.0e-5
  warmup_steps: 1000
  max_steps: 50000
  weight_decay: 0.1
  grad_clip: 1.0
  beta1: 0.9
  beta2: 0.95
  dataset: HuggingFaceFW/fineweb-edu
  tokenizer: gpt2
  log_interval: 10
  eval_interval: 500
  save_interval: 2500
  eval_steps: 100
