# GPT-2 124M Reference â€” reproduce original GPT-2 for baseline comparison
# Standard MHA + GELU FFN + LayerNorm

d_model: 768
n_layers: 12
vocab_size: 50257

attention:
  type: mha
  n_heads: 12
  dropout: 0.0

ffn:
  type: gelu
  dropout: 0.0

position:
  type: rope  # Upgrade from sinusoidal for fair comparison
  max_seq_len: 1024
  rope_base: 10000.0

norm:
  type: layernorm
  eps: 1.0e-5

output:
  type: tied

training:
  batch_size: 64
  seq_len: 1024
  lr: 6.0e-4
  min_lr: 6.0e-5
  warmup_steps: 1000
  max_steps: 50000
  weight_decay: 0.1
  grad_clip: 1.0
